{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthick47v2/question-generator/blob/main/data_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwJQYWpw0bKh"
      },
      "source": [
        "### Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fB30UaQqzMvZ"
      },
      "outputs": [],
      "source": [
        "# SQuAD dataset\n",
        "!wget https://data.deepai.org/squad1.1.zip\n",
        "!unzip squad1.1.zip\n",
        "\n",
        "# SciQ dataset\n",
        "!wget https://ai2-public-datasets.s3.amazonaws.com/sciq/SciQ.zip\n",
        "!unzip SciQ.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install third party libraries"
      ],
      "metadata": {
        "id": "RlGShTJnFhVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers==4.1.1\n",
        "!pip3 install tokenizers==0.9.4\n",
        "!pip3 install sentencepiece==0.1.94"
      ],
      "metadata": {
        "id": "6tMD73PQfpHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9r7FuwB0bKk"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dnArgwcUzUso"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from transformers import T5Tokenizer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzJ_kkvj0bKl"
      },
      "source": [
        "### Extract data from json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_json(filepath):\n",
        "  \"\"\"Load json file from storage.\n",
        "\n",
        "  Args:\n",
        "    filepath (str): Path of json file.\n",
        "\n",
        "  Returns:\n",
        "    list(dict(obj)): List of nested dictionaries.\n",
        "  \"\"\"\n",
        "  data = {}\n",
        "\n",
        "  with open(filepath) as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "JTy8CLImDnr5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SQuAD***\n",
        "\n",
        "- SQuAD dataset doesn't contain null values, so, no need to check.\n",
        "- We are only interested in generating questions from simple answers. So answers with more than 5 words will be filtered out."
      ],
      "metadata": {
        "id": "mftNffwYGEVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_from_squad(data):\n",
        "  \"\"\"Extract data from SQuAD dataset.\n",
        "\n",
        "  Args:\n",
        "    data (list(dict(obj))): List of nested dictionaries.\n",
        "\n",
        "  Returns:\n",
        "    tuple(list(str), list(str)): tuple of lists of model input and output. \n",
        "  \"\"\"\n",
        "  source = []\n",
        "  target = []\n",
        "\n",
        "  for topic in data['data']:\n",
        "    for dict_set in topic['paragraphs']:\n",
        "      for qna_set in dict_set['qas']:\n",
        "        if is_short_answer(qna_set['answers'][0]['text'], 5):\n",
        "          source.append(f\"context: {dict_set['context']} answer: {qna_set['answers'][0]['text']}\")\n",
        "          target.append(qna_set['question'])\n",
        "\n",
        "  return source, target"
      ],
      "metadata": {
        "id": "VSbdKr83F7pg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SciQ***\n",
        "\n",
        "- SCiQ dataset contains empty string for some values of `support` (mentioned in dataset readme.txt). So, that will be filtered out.\n",
        "- We are only interested in generating questions from simple answers. So answers with more than 5 words will be filtered out."
      ],
      "metadata": {
        "id": "L4p9xtIdGhxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_from_sciq(data):\n",
        "  \"\"\"Extract data from SciQ dataset.\n",
        "\n",
        "  Args:\n",
        "    data (list(dict(obj))): List of nested dictionaries.\n",
        "\n",
        "  Returns:\n",
        "    tuple(list(str), list(str)): tuple of lists of model input and output. \n",
        "  \"\"\"\n",
        "  source = []\n",
        "  target = []\n",
        "\n",
        "  for dict_set in data:\n",
        "    if dict_set['support'] == \"\":\n",
        "      continue\n",
        "    if is_short_answer(dict_set['correct_answer'], 5):\n",
        "      source.append(f\"context: {dict_set['support']} answer: {dict_set['correct_answer']}\")\n",
        "      target.append(dict_set['question'])\n",
        "\n",
        "  return source, target"
      ],
      "metadata": {
        "id": "HHpiOQSKC9bK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_short_answer(ans, threshold):\n",
        "  return len(ans.split()) <= threshold"
      ],
      "metadata": {
        "id": "EcPSX6I4WXaB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = parse_json('train-v1.1.json')\n",
        "squad_source_text, squad_target_text = extract_from_squad(data)\n",
        "\n",
        "sciq_source_text = []\n",
        "sciq_target_text = []\n",
        "\n",
        "for filename in ['train', 'test', 'valid']:\n",
        "  data = parse_json(f\"SciQ dataset-2 3/{filename}.json\")\n",
        "  source, target = extract_from_sciq(data)\n",
        "\n",
        "  sciq_source_text.extend(source)\n",
        "  sciq_target_text.extend(target)"
      ],
      "metadata": {
        "id": "-fJV1XwFKrPg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SQuAD***\n",
        "- Total data: 87,599\n",
        "- Filtered data: 76,135\n",
        "\n",
        "\n",
        "***SciQ***\n",
        "- Total data: 13,679\n",
        "- Filtered data: 12,214"
      ],
      "metadata": {
        "id": "gRVEcDRVOyYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data visualization and reduction"
      ],
      "metadata": {
        "id": "C-Phi5_CFusJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter out any duplicate questions."
      ],
      "metadata": {
        "id": "LtFxGHVCYqRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "squad_df = pd.DataFrame({'source_text': squad_source_text, 'target_text': squad_target_text})\n",
        "sciq_df = pd.DataFrame({'source_text': sciq_source_text, 'target_text': sciq_target_text})\n",
        "\n",
        "squad_df.drop_duplicates(subset=['target_text'], ignore_index=True, inplace=True)\n",
        "sciq_df.drop_duplicates(subset=['target_text'], ignore_index=True, inplace=True)"
      ],
      "metadata": {
        "id": "wndYSKhIZpSZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SQuAD***\n",
        "- Before: 76,135\n",
        "- After filtering out duplicates: 75,937\n",
        "\n",
        "\n",
        "***SciQ***\n",
        "- Before: 12,214\n",
        "- After filtering out duplicates: 12,133"
      ],
      "metadata": {
        "id": "DlBVR8ERal4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter out data with has exceeding tokens (than model input token size)"
      ],
      "metadata": {
        "id": "3Tc4nWaTh3ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')"
      ],
      "metadata": {
        "id": "XP92GWhybdNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_token_count(df, dataset):\n",
        "  \"\"\"Plot token count againts no of data.\n",
        "  \n",
        "  Args:\n",
        "    df (DataFrame): DataFrame of dataset that needs to plot.\n",
        "    dataset (str): Dataset name.\n",
        "  \"\"\"\n",
        "  source_token_count = []\n",
        "  target_token_count = []\n",
        "\n",
        "  for _, row in df.iterrows():\n",
        "    source_token_count.append(len(t5_tokenizer.encode(row['source_text'])))\n",
        "    target_token_count.append(len(t5_tokenizer.encode(row['target_text'])))\n",
        "\n",
        "  fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,10))\n",
        "  \n",
        "  sns.histplot(source_token_count, ax=ax1).set(title=f\"{dataset}-'source_text'-tokens\")\n",
        "  sns.histplot(target_token_count, ax=ax2).set(title=f\"{dataset}-'target_text'-tokens\")"
      ],
      "metadata": {
        "id": "aczVq9_tbBo8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_out_tokens(df, filter_by, max_token_len,):\n",
        "  \"\"\"Delete rows which has data with exceeding token length.\n",
        "\n",
        "  Args:\n",
        "    df (DataFrame): Dataset that needs to be processed.\n",
        "    filter_by (str): Name of attribute, to check token length.\n",
        "    max_token_len (int): Maximum token length (For model input = 512).\n",
        "\n",
        "  Returns:\n",
        "    (DataFrame): Filtered dataset by 'filter_by' attribute\n",
        "  \"\"\"\n",
        "  df['not_exceeded'] = df.apply(lambda x: len(t5_tokenizer.encode(x[filter_by])) <= max_token_len, axis=1)\n",
        "  df = df[df['not_exceeded']]\n",
        "  return df.drop(columns=['not_exceeded'])"
      ],
      "metadata": {
        "id": "YVipA1jOkgyG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_df_by_token_len(df, dataset,filter_by, max_token_len):\n",
        "  \"\"\"Filter dataset against model requirements.\n",
        "\n",
        "  Args:\n",
        "    df (DataFrame): Dataset that needs to be processed.\n",
        "    dataset (str): Dataset name.\n",
        "    filter_by (str): Name of attribute, to check against.\n",
        "    max_token_len (int): Maximum token length (For model input = 512).\n",
        "\n",
        "  Returns:\n",
        "    (DataFrame): Filtered dataset by 'filter_by' attribute\n",
        "  \"\"\"\n",
        "  print(f\"filter by {filter_by}\")\n",
        "  print('Before filtering...')\n",
        "  plot_token_count(df, dataset)\n",
        "  df = filter_out_tokens(df, filter_by, max_token_len)\n",
        "  print('After filtering...')\n",
        "  plot_token_count(df, dataset)\n",
        "  return df"
      ],
      "metadata": {
        "id": "e2DcCwu28u4T"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SciQ***"
      ],
      "metadata": {
        "id": "tpSlTj5QHMVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sciq_df = filter_df_by_token_len(sciq_df, 'SciQ', 'source_text', 512)"
      ],
      "metadata": {
        "id": "13023vFiCRiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since most of 'target_text' token lengths are between 0-6x, filter out the outliers. (Used to set max out token length in Model training)\n",
        "\n",
        "Let's filtered out by 72.\n"
      ],
      "metadata": {
        "id": "B95oLby3HaEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sciq_df = filter_df_by_token_len(sciq_df, 'SciQ', 'target_text', 72)"
      ],
      "metadata": {
        "id": "zOYnVvePjI-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SQuAD***"
      ],
      "metadata": {
        "id": "q4UC_NrwH82l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "squad_df = filter_df_by_token_len(squad_df, 'SQuAD', 'source_text', 512)"
      ],
      "metadata": {
        "id": "V1FSb2KCxp-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since most of 'target_text' token lengths are between 0-4x, filter out the outliers. (Used to set max out token length in Model training)\n",
        "\n",
        "Let's filtered out by 48."
      ],
      "metadata": {
        "id": "NpxxKCCoICNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> We aren't combining both dataset for training. Each one will be trained separately for different purpose. So, differ in max output length doesn't matter."
      ],
      "metadata": {
        "id": "FI0xl8ENIP93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "squad_df = filter_df_by_token_len(squad_df, 'SQuAD', 'target_text', 48)"
      ],
      "metadata": {
        "id": "3eAjTolNEw9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sciq_df.shape, squad_df.shape"
      ],
      "metadata": {
        "id": "HH2rH6FS6Fqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SQuAD***\n",
        "- Before: 75,937\n",
        "- After filtering out data w exceeding token lens: 75,699\n",
        "\n",
        "\n",
        "***SciQ***\n",
        "- Before: 12,133\n",
        "- After filtering out data w exceeding token lens: 11,970"
      ],
      "metadata": {
        "id": "rPMqCgHX6QCj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyKaYJnF0bKr"
      },
      "source": [
        "### Export as *.csv and upload to GDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7g2Ab4aC9HT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMnlCw4DEXMA"
      },
      "outputs": [],
      "source": [
        "sciq_df.to_csv('SciQ-processed.csv', index=False)\n",
        "squad_df.to_csv('SQuAD-processed.csv', index=False)\n",
        "\n",
        "!mv SciQ-processed.csv SQuAD-processed.csv gdrive/MyDrive/mcq-gen"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Leb_86wyMnr6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "squad-data-extraction.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}