{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthick47v2/question-generator/blob/main/data_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwJQYWpw0bKh"
      },
      "source": [
        "### Download dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fB30UaQqzMvZ"
      },
      "outputs": [],
      "source": [
        "# SQuAD dataset\n",
        "!wget https://data.deepai.org/squad1.1.zip\n",
        "!unzip squad1.1.zip\n",
        "\n",
        "# SciQ dataset\n",
        "!wget https://ai2-public-datasets.s3.amazonaws.com/sciq/SciQ.zip\n",
        "!unzip SciQ.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlGShTJnFhVh"
      },
      "source": [
        "### Install third party libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tMD73PQfpHG"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers == 4.1.1\n",
        "!pip3 install tokenizers == 0.9.4\n",
        "!pip3 install sentencepiece == 0.1.94\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9r7FuwB0bKk"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dnArgwcUzUso"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from transformers import T5Tokenizer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzJ_kkvj0bKl"
      },
      "source": [
        "### Extract data from json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JTy8CLImDnr5"
      },
      "outputs": [],
      "source": [
        "def parse_json(filepath):\n",
        "    \"\"\"Load json file from storage.\n",
        "\n",
        "    Args:\n",
        "      filepath (str): Path of json file.\n",
        "\n",
        "    Returns:\n",
        "      list(dict(obj)): List of nested dictionaries.\n",
        "    \"\"\"\n",
        "    data = {}\n",
        "\n",
        "    with open(filepath) as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mftNffwYGEVB"
      },
      "source": [
        "**_SQuAD_**\n",
        "\n",
        "- SQuAD dataset doesn't contain null values, so, no need to check.\n",
        "- We are only interested in generating questions from simple answers. So answers with more than 5 words will be filtered out.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VSbdKr83F7pg"
      },
      "outputs": [],
      "source": [
        "def extract_from_squad(data):\n",
        "    \"\"\"Extract data from SQuAD dataset.\n",
        "\n",
        "    Args:\n",
        "      data (list(dict(obj))): List of nested dictionaries.\n",
        "\n",
        "    Returns:\n",
        "      tuple(list(str), list(str)): tuple of lists of model input and output. \n",
        "    \"\"\"\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "\n",
        "    for topic in data['data']:\n",
        "        for dict_set in topic['paragraphs']:\n",
        "            for qna_set in dict_set['qas']:\n",
        "                if is_short_answer(qna_set['answers'][0]['text'], 5):\n",
        "                    contexts.append(f\"context: {dict_set['context']}\")\n",
        "                    questions.append(f\"question: {qna_set['question']}\")\n",
        "                    answers.append(f\"answer: {qna_set['answers'][0]['text']}\")\n",
        "\n",
        "    return contexts, questions, answers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4p9xtIdGhxg"
      },
      "source": [
        "**_SciQ_**\n",
        "\n",
        "- SCiQ dataset contains empty string for some values of `support` (mentioned in dataset readme.txt). So, that will be filtered out.\n",
        "- We are only interested in generating questions from simple answers. So answers with more than 5 words will be filtered out.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HHpiOQSKC9bK"
      },
      "outputs": [],
      "source": [
        "def extract_from_sciq(data):\n",
        "    \"\"\"Extract data from SciQ dataset.\n",
        "\n",
        "    Args:\n",
        "      data (list(dict(obj))): List of nested dictionaries.\n",
        "\n",
        "    Returns:\n",
        "      tuple(list(str), list(str)): tuple of lists of model input and output. \n",
        "    \"\"\"\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "\n",
        "    for dict_set in data:\n",
        "        if dict_set['support'] == \"\":\n",
        "            continue\n",
        "        if is_short_answer(dict_set['correct_answer'], 5):\n",
        "            contexts.append(f\"context: {dict_set['support']}\")\n",
        "            questions.append(f\"question: {dict_set['question']}\")\n",
        "            answers.append(f\"answer: {dict_set['correct_answer']}\")\n",
        "\n",
        "    return contexts, questions, answers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EcPSX6I4WXaB"
      },
      "outputs": [],
      "source": [
        "def is_short_answer(ans, threshold):\n",
        "    return len(ans.split()) <= threshold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-fJV1XwFKrPg"
      },
      "outputs": [],
      "source": [
        "data = parse_json('train-v1.1.json')\n",
        "squad_contexts, squad_questions, squad_answers = extract_from_squad(data)\n",
        "\n",
        "sciq_contexts = []\n",
        "sciq_questions = []\n",
        "sciq_answers = []\n",
        "\n",
        "for filename in ['train', 'test', 'valid']:\n",
        "    data = parse_json(f\"SciQ dataset-2 3/{filename}.json\")\n",
        "    contexts, questions, answers = extract_from_sciq(data)\n",
        "\n",
        "    sciq_contexts.extend(contexts)\n",
        "    sciq_questions.extend(questions)\n",
        "    sciq_answers.extend(answers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRVEcDRVOyYb"
      },
      "source": [
        "**_SQuAD_**\n",
        "\n",
        "- Total data: 87,599\n",
        "- Filtered data: 76,135\n",
        "\n",
        "**_SciQ_**\n",
        "\n",
        "- Total data: 13,679\n",
        "- Filtered data: 12,214\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-Phi5_CFusJ"
      },
      "source": [
        "### Data visualization and reduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtFxGHVCYqRm"
      },
      "source": [
        "Filter out any duplicate questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wndYSKhIZpSZ"
      },
      "outputs": [],
      "source": [
        "squad_df = pd.DataFrame({'context': squad_contexts, 'question': squad_questions,\n",
        "                         'answer': squad_answers})\n",
        "sciq_df = pd.DataFrame({'context': sciq_contexts, 'question': sciq_questions,\n",
        "                        'answer': sciq_answers})\n",
        "\n",
        "squad_df.drop_duplicates(subset=['question'], ignore_index=True, inplace=True)\n",
        "sciq_df.drop_duplicates(subset=['question'], ignore_index=True, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlBVR8ERal4M"
      },
      "source": [
        "**_SQuAD_**\n",
        "\n",
        "- Before: 76,135\n",
        "- After filtering out duplicates: 75,937\n",
        "\n",
        "**_SciQ_**\n",
        "\n",
        "- Before: 12,214\n",
        "- After filtering out duplicates: 12,133\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tc4nWaTh3ZR"
      },
      "source": [
        "Filter out data with has exceeding tokens (than model input token size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP92GWhybdNt"
      },
      "outputs": [],
      "source": [
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ahYtwc8eAUt"
      },
      "outputs": [],
      "source": [
        "# SAMPLE tokenization with padding ...\n",
        "encoding = t5_tokenizer(sciq_df.loc[0, 'context'],\n",
        "                        sciq_df.loc[0, 'answer'], max_length=512,\n",
        "                        padding='max_length', truncation='only_first',\n",
        "                        add_special_tokens=True, return_attention_mask=True,\n",
        "                        return_tensors='pt')\n",
        "preds = [\n",
        "    t5_tokenizer.decode(\n",
        "        input_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    for input_id in encoding['input_ids']\n",
        "]\n",
        "\n",
        "\" \".join(preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aczVq9_tbBo8"
      },
      "outputs": [],
      "source": [
        "def plot_token_count(df, dataset):\n",
        "    \"\"\"Plot token count againts no of data.\n",
        "\n",
        "    Args:\n",
        "      df (DataFrame): DataFrame of dataset that needs to plot.\n",
        "      dataset (str): Dataset name.\n",
        "    \"\"\"\n",
        "    source_token_count = []\n",
        "    target_token_count = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        source_token_count.append(get_token_len(row['context'],\n",
        "                                                text_pair=row['answer']))\n",
        "        target_token_count.append(get_token_len(row['question']))\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "    sns.histplot(source_token_count, ax=ax1).set(\n",
        "        title=f\"{dataset}-source-tokens\")\n",
        "    sns.histplot(target_token_count, ax=ax2).set(\n",
        "        title=f\"{dataset}-target-tokens\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9SDlEPVoiq_i"
      },
      "outputs": [],
      "source": [
        "def get_token_len(text, text_pair=None):\n",
        "    \"\"\"Get length of tokens\n",
        "\n",
        "    Args:\n",
        "      text (str): 1st input sequence.\n",
        "      text_pair (str): 2nd input sequence. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "      (int): Length of tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    return len(t5_tokenizer(text, text_pair)['input_ids'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YVipA1jOkgyG"
      },
      "outputs": [],
      "source": [
        "def filter_out_tokens(df, max_token_len, text, text_pair):\n",
        "    \"\"\"Delete rows which has data with exceeding token length.\n",
        "\n",
        "    Args:\n",
        "      df (DataFrame): Dataset that needs to be processed.\n",
        "      max_token_len (int): Maximum token length (For model input = 512).\n",
        "      text (str): 1st input sequence.\n",
        "      text_pair (str): 2nd input sequence.\n",
        "\n",
        "    Returns:\n",
        "      (DataFrame): Filtered dataset by 'filter_by' attribute\n",
        "    \"\"\"\n",
        "    df['not_exceeded'] = df.apply(\n",
        "        lambda x: get_token_len(x[text], text_pair) <= max_token_len, axis=1)\n",
        "    df = df[df['not_exceeded']]\n",
        "    return df.drop(columns=['not_exceeded'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "e2DcCwu28u4T"
      },
      "outputs": [],
      "source": [
        "def filter_df_by_token_len(df, dataset, max_token_len, text, text_pair=None):\n",
        "    \"\"\"Filter dataset against model requirements.\n",
        "\n",
        "    Args:\n",
        "      df (DataFrame): Dataset that needs to be processed.\n",
        "      dataset (str): Dataset name.\n",
        "      max_token_len (int): Maximum token length (For model input = 512).\n",
        "      text (str): 1st input sequence.\n",
        "      text_pair (str): 2nd input sequence. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "      (DataFrame): Filtered dataset by 'filter_by' attribute\n",
        "    \"\"\"\n",
        "    print(f\"filter by {text}\")\n",
        "    print('Before filtering...')\n",
        "    plot_token_count(df, dataset)\n",
        "    df = filter_out_tokens(df, max_token_len, text, text_pair)\n",
        "    print('After filtering...')\n",
        "    plot_token_count(df, dataset)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpSlTj5QHMVN"
      },
      "source": [
        "**_SciQ_**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13023vFiCRiJ"
      },
      "outputs": [],
      "source": [
        "sciq_df = filter_df_by_token_len(sciq_df, 'SciQ', 512, 'context', 'answer')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B95oLby3HaEb"
      },
      "source": [
        "Since most of 'target_text' token lengths are between 0-6x, filter out the outliers. (Used to set max out token length in Model training)\n",
        "\n",
        "Let's filter it out by 72.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOYnVvePjI-t"
      },
      "outputs": [],
      "source": [
        "sciq_df = filter_df_by_token_len(sciq_df, 'SciQ', 72, 'question')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4UC_NrwH82l"
      },
      "source": [
        "**_SQuAD_**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1FSb2KCxp-z"
      },
      "outputs": [],
      "source": [
        "squad_df = filter_df_by_token_len(squad_df, 'SQuAD', 512, 'context', 'answer')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpxxKCCoICNT"
      },
      "source": [
        "Since most of 'target_text' token lengths are between 0-4x, filter out the outliers. (Used to set max out token length in Model training)\n",
        "\n",
        "Let's filter it out by 48.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI0xl8ENIP93"
      },
      "source": [
        "> We aren't combining both dataset for training. Each one will be trained separately for different purpose. So, differ in max output length doesn't matter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eAjTolNEw9n",
        "outputId": "64b9e251-b64c-4244-ac6b-518e8c2a5f5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "filter by question\n",
            "Before filtering...\n",
            "After filtering...\n"
          ]
        }
      ],
      "source": [
        "squad_df = filter_df_by_token_len(squad_df, 'SQuAD', 48, 'question')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPMqCgHX6QCj"
      },
      "source": [
        "**_SQuAD_**\n",
        "\n",
        "- Before: 75,937\n",
        "- After filtering out data w exceeding token lens: 75,711\n",
        "\n",
        "**_SciQ_**\n",
        "\n",
        "- Before: 12,133\n",
        "- After filtering out data w exceeding token lens: 11,973\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyKaYJnF0bKr"
      },
      "source": [
        "### Export as \\*.csv and upload to GDrive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7g2Ab4aC9HT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "bMnlCw4DEXMA"
      },
      "outputs": [],
      "source": [
        "sciq_df.to_csv('SciQ-processed.csv', index=False)\n",
        "squad_df.to_csv('SQuAD-processed.csv', index=False)\n",
        "\n",
        "!mv SciQ-processed.csv SQuAD-processed.csv gdrive/MyDrive/mcq-gen\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "squad-data-extraction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
